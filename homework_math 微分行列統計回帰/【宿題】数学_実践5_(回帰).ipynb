{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMUhlLfNafT8"
      },
      "outputs": [],
      "source": [
        "# 実践5-1(回帰)\n",
        "# 最小二乗法について説明してください。\n",
        "sum(予測値ー実測値)² を限りなく0に近づける手法。\n",
        "\n",
        "f(x) = ax + b\n",
        "xに値を代入し、\n",
        " (a*2 + b - 6(x=2時の実測値))² + (a*5 + b - 13(x=5時の実測値))² + ... = 0\n",
        "上記が限りなく0に近づくaとbの値を見つける。\n",
        "\n",
        "【最小値化したい対象の関数(f(x))】\n",
        "目的関数、損失関数、誤差関数など。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 実践5-2:\n",
        "# 過学習について説明してください。 過学習が起きると何が問題でしょうか。\n",
        "\n",
        "学習データ量不足や既知データに過度にフィットさせ複雑化しすぎたことにより、\n",
        "手元のデータの外れ値にもピッタリ合ったモデルを構築してしまい予測データの精度を下げる。"
      ],
      "metadata": {
        "id": "BopyQoKRaqi5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 実践5-3:\n",
        "# 決定係数R^2 を使う場面について説明してください。\n",
        "\n",
        "回帰モデルにおいて、元データを学習データとテストデータに分け、\n",
        "学習データによって構築した予測モデル結果とテストデータの差を0-1の指標で表すのが決定係数。\n",
        "\n",
        "【目安】\n",
        "0.5以上 ある程度の予測が当てはまると認められる。\n",
        "0.6以上 有用な回帰式が得られたと判断されることが多い。\n",
        "0.8以上 非常に優秀なモデル。\n",
        "0.9以上 過学習の可能性がある。"
      ],
      "metadata": {
        "id": "Vrx3r6WSauPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 実践5-4:\n",
        "#以下の決定係数の算出問題を計算してください。\n",
        "\n",
        "# x (年収/万円): 300, 400, 500, 600, 700\n",
        "# y (支出額/万円): 150, 220, 300, 380, 420\n",
        "\n",
        "import math\n",
        "\n",
        "# データ\n",
        "x = [300, 400, 500, 600, 700]\n",
        "y = [150, 220, 300, 380, 420]\n",
        "n = len(x)\n",
        "\n",
        "# 平均値\n",
        "x_mean = sum(x) / n\n",
        "y_mean = sum(y) / n\n",
        "\n",
        "# 直線の係数 a, b (最小二乗法)\n",
        "numer = sum((x_i - x_mean) * (y_i - y_mean) for x_i, y_i in zip(x, y))\n",
        "denom = sum((x_i - x_mean) ** 2 for x_i in x)\n",
        "a = numer / denom\n",
        "b = y_mean - a * x_mean\n",
        "\n",
        "# 予測値\n",
        "y_pred = [a * x_i + b for x_i in x]\n",
        "\n",
        "# 決定係数\n",
        "# R² = (予測値y-実測値yのずれ)²の合計 / (実測値y-平均yのずれ)²の合計\n",
        "ss_res = sum((y_i - y_actual_value) ** 2 for y_i, y_actual_value in zip(y, y_pred))\n",
        "ss_tot = sum((y_i - y_mean) ** 2 for y_i in y)\n",
        "r_squared = 1 - (ss_res / ss_tot)\n",
        "\n",
        "print(f\"直線の係数: a = {a}, b = {b}\")\n",
        "print(f\"決定係数 R^2 = {r_squared}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-VfMYUYayFg",
        "outputId": "86847c78-64ad-4c75-e2e4-7251b8b17f8c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "直線の係数: a = 0.7, b = -56.0\n",
            "決定係数 R^2 = 0.9894991922455574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 実践5-5\n",
        "# 単回帰と重回帰の違いを数式を例にして説明してください。\n",
        "\n",
        "# 単回帰\n",
        "f(x) = ax + b\n",
        "# 重回帰\n",
        "f(x,z) = ax + bz + c"
      ],
      "metadata": {
        "id": "0eGFP_una4x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "実践5-6\n",
        "リッジ回帰・ラッソ回帰・elastic-netの違いについて調べて報告してください。\n",
        "例えば説明変数の数が多い場合はどの手法を使うべきでしょうか？\n",
        "\n",
        "線形回帰モデルは、説明変数(特微量)の数が多いと\n",
        "過学習や多重共線性(変数同士の相関が強すぎる)が起きやすくなる。\n",
        "それを防ぐために最小二乗法に加えて正則化という手法を加える。\n",
        "\n",
        "■リッジ回帰\n",
        "【特徴】全ての説明変数を使用し、αを大きくするほど係数が小さくなり過学習を防ぐ。\n",
        "【強み】説明変数が多く、全ての変数が予測に影響している場合や説明変数間に強すぎる相関がある場合。\n",
        "【弱み】重要でない説明変数を削除できない。\n",
        "【例】　多変量な生体データ(バイタルサインや血液検査)からの死亡リスク予測、ゲノムデータのような多数の説明変数を用いる場合\n",
        "　　　→変数間の相関があっても安定的な予測が可能、全ての変数を活かして精度重視の予測モデルを作りたい時。中～多変量、予測重視。\n",
        "\n",
        "■ラッソ回帰\n",
        "【特徴】説明変数の係数を0にできるので、不要な説明変数を除外できる。\n",
        "【強み】説明変数が多いが、重要な説明変数は一部の変数だけな場合。\n",
        "【弱み】説明変数間に高い相関があると、どの変数を選択するかが不安定になることがある。\n",
        "【例】　診断モデルで最小限の特微量を発見したい時、コストや検査負担を減らすために必要な検査項目に絞りたい時\n",
        "　　　→医師が納得しやすい、少ない変数(検査)でも良いモデルが作れる。小～中規模データ、解釈重視。\n",
        "\n",
        "■Elastic Net\n",
        "【特徴】リッジ回帰とラッソ回帰の両方の特性を持つ。\n",
        "【強み】説明変数が多く、変数間に高い相関がある場合やラッソ回帰では変数選択が不安定になる場合。\n",
        "【弱み】ハイパーﾊﾟﾗﾒｰﾀが2つ(L1とL2の比率、正則化が強い)あり、調整が難しい。モデルの解釈が複雑になりやすい。\n",
        "【例】　ゲノムや遺伝子発現データ(数千～数万の特微量)、画像診断データからの特微量抽出後の分析\n",
        "　　　　がん患者の遺伝子発現データから治療効果が出そうなバイオマーカーを選ぶ\n",
        "　　　→高次元医療データに最もよく使われる。高次元、相関有り、スパース性も欲しい場合。"
      ],
      "metadata": {
        "id": "KpXRL-Wya5ZK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}